2026-01-06 13:20:07,862 - __main__ - INFO - Fetching 100 articles from GDELT (country=sweden)...
2026-01-06 13:20:07,862 - ml.ingestion.script - INFO - Fetching 100 articles from sweden in 1 batches
2026-01-06 13:20:07,862 - ml.ingestion.script - INFO - Batch 1/1: timespan=7d, batch_size=100
2026-01-06 13:20:09,428 - ml.ingestion.script - INFO - Batch 1: Got 100 articles, unique so far: 100
2026-01-06 13:20:09,428 - ml.ingestion.script - INFO - Total unique articles collected: 100
2026-01-06 13:20:09,428 - __main__ - INFO - âœ“ Fetched 100 articles
2026-01-06 13:20:09,428 - __main__ - INFO - Loading LLM: AI-Sweden-Models/gpt-sw3-1.3b-instruct...
Traceback (most recent call last):
  File "/Users/juozas/Documents/Projects/the-daily-collage/ml/utils/generate_templates.py", line 413, in <module>
    main()
  File "/Users/juozas/Documents/Projects/the-daily-collage/ml/utils/generate_templates.py", line 402, in main
    generate_templates_and_keywords(
  File "/Users/juozas/Documents/Projects/the-daily-collage/ml/utils/generate_templates.py", line 299, in generate_templates_and_keywords
    llm = TemplateLLM(model_name=llm_model_name)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/juozas/Documents/Projects/the-daily-collage/ml/utils/generate_templates.py", line 57, in __init__
    self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/juozas/Documents/Projects/the-daily-collage/venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 1156, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/juozas/Documents/Projects/the-daily-collage/venv/lib/python3.12/site-packages/transformers/utils/import_utils.py", line 2157, in __getattribute__
    requires_backends(cls, cls._backends)
  File "/Users/juozas/Documents/Projects/the-daily-collage/venv/lib/python3.12/site-packages/transformers/utils/import_utils.py", line 2143, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
GPTSw3Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the
installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

